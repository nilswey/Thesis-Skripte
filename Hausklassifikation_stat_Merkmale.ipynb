{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ed70b5b-45be-4f58-a9a4-b7644235b673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSkript  zur Statistische Merkmalsberechnung für die Häuserklassifikation nach Hecht (2014)\\nAuthor: Nils Weyrauch\\n\\nPackages:\\nGeopandas (+Dependencies)\\nPandas\\nShapely\\nfiona\\nrasterio\\nnumpy\\n\\nAufbau:\\n1. Defintion der Funktionen für die Berechnung der Merkmale\\n2. Integration der Input Daten \\n3. Berechnung der Merkmale mit Nebengebäude\\n4. Berechnung der Merkmale ohne Nebengebäude\\n5. Export der Daten \\n\\n\\nDaten: (zuletzt überprüft: 19.04.2024)\\nALKIS Grundrissdaten im NAS Format: https://www.opengeodata.nrw.de/produkte/geobasis/lk/akt/gru_xml/\\n(anschließende Umwandlung in Shape Dateien)\\nGebäudereferenzen im txt Format: https://www.opengeodata.nrw.de/produkte/geobasis/lk/akt/gebref_txt/\\n(anschließende Umwandlung zu Shape Datei)\\nDigitale Verwaltungsgrenzen NRW: https://www.opengeodata.nrw.de/produkte/geobasis/vkg/dvg/dvg1/\\nNormalisiertes Digitales Oberflächenmodell: https://www.opengeodata.nrw.de/produkte/geobasis/hm/ndom50_tiff/ndom50_tiff/\\n(Abfrage über Geoporatl NRW, für die Landkreise: Wuppertal, Remscheid, Ennepe Ruhr Kreis, Oberbergischer Kreis || ca. 2500 Einzelkacheln)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Skript  zur Statistische Merkmalsberechnung für die Häuserklassifikation nach Hecht (2014)\n",
    "Author: Nils Weyrauch\n",
    "\n",
    "Packages:\n",
    "Geopandas (+Dependencies)\n",
    "Pandas\n",
    "Shapely\n",
    "fiona\n",
    "rasterio\n",
    "numpy\n",
    "\n",
    "Aufbau:\n",
    "1. Defintion der Funktionen für die Berechnung der Merkmale\n",
    "2. Integration der Input Daten \n",
    "3. Berechnung der Merkmale mit Nebengebäude\n",
    "4. Berechnung der Merkmale ohne Nebengebäude\n",
    "5. Export der Daten \n",
    "\n",
    "\n",
    "Daten: (zuletzt überprüft: 19.04.2024)\n",
    "ALKIS Grundrissdaten im NAS Format: https://www.opengeodata.nrw.de/produkte/geobasis/lk/akt/gru_xml/\n",
    "(anschließende Umwandlung in Shape Dateien)\n",
    "Gebäudereferenzen im txt Format: https://www.opengeodata.nrw.de/produkte/geobasis/lk/akt/gebref_txt/\n",
    "(anschließende Umwandlung zu Shape Datei)\n",
    "Digitale Verwaltungsgrenzen NRW: https://www.opengeodata.nrw.de/produkte/geobasis/vkg/dvg/dvg1/\n",
    "Normalisiertes Digitales Oberflächenmodell: https://www.opengeodata.nrw.de/produkte/geobasis/hm/ndom50_tiff/ndom50_tiff/\n",
    "(Abfrage über Geoporatl NRW, für die Landkreise: Wuppertal, Remscheid, Ennepe Ruhr Kreis, Oberbergischer Kreis || ca. 2500 Einzelkacheln)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "048ddb11-f307-47a7-81dd-4322a0344283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sqrt, pi\n",
    "import geopandas as gpd\n",
    "import shapely\n",
    "from pathlib import Path\n",
    "import fiona \n",
    "import pandas as pd\n",
    "import shapely.geometry \n",
    "import rasterio \n",
    "from rasterio.mask import mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffda9360-2d38-483e-afb1-55061a05abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieren der Funktionen für spätere Berechnung der statistischen Merkmale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e054c3b2-5891-4ae3-821a-e119c9ce3c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funktion um die Anzahl aller benachbarten Einzelgebäude festzustellen\n",
    "Merkmal: SUM_AnzPo\n",
    "\n",
    "Aufbau: \n",
    "1. Alle überschneidenden Polygone (\"intersection\") in Dictionary festhalten\n",
    "2. Zähler anlegen, welcher für jede gemeinsame Kante (\"touch\" Operator) um eins nach oben geht\n",
    "3. Werte zurückgeben an IP GDF\n",
    "\"\"\"\n",
    "\n",
    "def alt_Sum_AnzPo(gdf, col_name): \n",
    "\n",
    "    # Anlegen eines spatial index für schnellere computation\n",
    "    spatial_index = gdf.sindex  \n",
    "    \n",
    "    # Erstellen eines Dictionaries, hier wird die Anzahl der benachbarten Polygone pro index festgehalten\n",
    "    neighbor_counts = {}\n",
    "\n",
    "    # Loop über jede Zeile des GDF\n",
    "    for index, polygon in gdf.iterrows(): \n",
    "        \n",
    "        # Der Spatial index wird auf überlappung getestet,\n",
    "        # alle überlappenden indices werden festgehalten, zum check ob Kanten sich berühren\n",
    "        # spart Zeit gegenüber check von allen Polygonen auf gleiche Kanten\n",
    "        inters_idx = list(spatial_index.intersection(polygon.geometry.bounds))\n",
    "        \n",
    "        # Aussortieren des zu überprüfenden Polygons zur Fehlervermeidung\n",
    "        nearby_indices = [i for i in inters_idx if i != index]\n",
    "        \n",
    "        # Variable für Zählen wird festgehalten\n",
    "        neighbor_count = 0\n",
    "        \n",
    "        # Geometrien werden, auf Basis des Indexes aus Liste inters_pol, aus input Geodataframe aufgerufen,\n",
    "        # für die Geometrien wird auf \"touch\" Operation gecheckt (= check auf gleiche Kante)\n",
    "        # falls Ergebis True steigt der counter um 1\n",
    "        # falls keine überlappenden Polygone gefunden wurden wird die 0 übertragen\n",
    "        for i in nearby_indices:\n",
    "            nearby_polygon = gdf.loc[i, 'geometry']\n",
    "            if polygon.geometry.touches(nearby_polygon):\n",
    "                neighbor_count += 1\n",
    "        \n",
    "        # Das Wertepaar index : Nachbarschaftscount wird in ursprünglichem Dictionary festgehalten\n",
    "        neighbor_counts[index] = neighbor_count\n",
    "\n",
    "    # \"Nachbarn\" Spalte des ursprünglichen Dataframes wird anhand des indexes geupdated\n",
    "    gdf[col_name] = gdf.index.map(neighbor_counts.get)\n",
    "\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90a7c522-d813-4a1e-9f21-47adccdf7e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funktion um die Anzahl aller benachbarten Einzelgebäude festzustellen ohne Einbezug der Nebengebäude\n",
    "Benötigt vorherige Berechnung der Hauskoordinatenanzahl für die Filterung\n",
    "\n",
    "Merkmal: w_Sum_AnzPo\n",
    "\n",
    "Aufbau:\n",
    "selbes Prinzip wie SUM_AnzPo, nur mit vorheriger Filterung der Nebengebäude\n",
    "Index Reset nötig durch lückenhafte Indizierung für Loop\n",
    "\"\"\"\n",
    "\n",
    "def w_Sum_AnzPo(gdf, col_name): \n",
    "\n",
    "    # Filtern der Nebengebäude\n",
    "    gdf_sel = gdf[gdf[\"HK_ANZ\"] >= 1]\n",
    "\n",
    "    #Index wird Resetted da, durch Filterung index Lücken aufweist, führt sonst zu fehler\n",
    "    gdf_sel = gdf_sel.reset_index(names=\"orig_id\")\n",
    "\n",
    "    # Anlegen eines spatial index für schnellere computation\n",
    "    spatial_index = gdf_sel.sindex  \n",
    "    \n",
    "    # Erstellen eines Dictionaries, hier wird die Anzahl der benachbarten Polygone pro index festgehalten\n",
    "    neighbor_counts = {}\n",
    "\n",
    "    # Loop über jede Zeile des GDF\n",
    "    for index, polygon in gdf_sel.iterrows(): \n",
    "        \n",
    "        # Der Spatial index wird auf überlappung getestet,\n",
    "        # alle überlappenden indices werden festgehalten, zum check ob Kanten sich berühren\n",
    "        # spart Zeit gegenüber direktem check auf gleiche Kanten\n",
    "        inters_idx = list(spatial_index.intersection(polygon.geometry.bounds))\n",
    "        \n",
    "        # Aussortieren des zu überprüfenden Polygons zur Fehlervermeidung\n",
    "        nearby_indices = [i for i in inters_idx if i != index]\n",
    "        \n",
    "        # Variable für Zählen wird festgehalten, Grundlage für counter\n",
    "        neighbor_count = 0\n",
    "        \n",
    "        # Geometrien werden, auf Basis des Indexes aus Liste inters_pol, aus input Geodataframe aufgerufen,\n",
    "        # für die abgerufenen Indeces, wird auf \"touch\" Operation gecheckt\n",
    "        # falls Ergebis True steigt der counter um 1\n",
    "        # falls keine überlappenden Polygone gefunden wurden wird die 0 übertragen\n",
    "        for i in nearby_indices:\n",
    "            nearby_polygon = gdf_sel.loc[i, 'geometry']\n",
    "            if polygon.geometry.touches(nearby_polygon):\n",
    "                neighbor_count += 1\n",
    "        \n",
    "        # Das Wertepaar index : Nachbarschaftscount wird in ursprünglichem Dictionary festgehalten\n",
    "        neighbor_counts[index] = neighbor_count\n",
    "\n",
    "    # \"Nachbarn\" Spalte des selektierten Dataframes wird anhand des indexes geupdated\n",
    "    gdf_sel[col_name] = gdf_sel.index.map(neighbor_counts.get)\n",
    "\n",
    "    # um die Werte auf den Input GDF zurückzumergen werden orig_id und Werte zurückgemergt auf die orig_id\n",
    "    gdf = gdf.merge(gdf_sel[[\"orig_id\", col_name]], left_index=True, right_on=\"orig_id\", how=\"left\")\n",
    "\n",
    "    #droppen der Hilfsspalte\n",
    "    gdf.drop(columns=[\"orig_id\"], inplace=True)\n",
    "   \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ccc79b5-5dc1-4e01-b57e-ef494eecefd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funktion welche die Größe der Polygone kalkuliert\n",
    "\n",
    "Merkmal: Area, Length\n",
    "\"\"\"\n",
    "\n",
    "def calc_area_length(gdf): \n",
    "\n",
    "    #Errechnet die Größe und den Umfang des Polygons auf Basis der Geometry Spalte und fügt sie diesem wieder hinzu\n",
    "    gdf[\"area\"] = gdf[\"geometry\"].area \n",
    "    gdf[\"length\"] = gdf[\"geometry\"].length\n",
    "    \n",
    "    return gdf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "253b5129-0320-4e74-9692-82b1eb2332f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funktion um die Einzelgebäude zu Regionen zusammenfassen, \n",
    "dissagregiert die einzelnen Geometrien zusammen von allen baulich zusammenhängenden Gebäuden\n",
    "\n",
    "Aufbau:\n",
    "1. Auflösen der Gebäudegeometrien von allen baulich zusammenhängenden Gebäuden (vgl. \"Dissolve\"-Werkzeug GIS)\n",
    "2. Berechnung der Größe der neuen Polygone (Gebäuderegionen)\n",
    "3. Berechnung der Richardson Compactness für die Regionen\n",
    "3. Rückgabe der Gebäuderegionen, herausfiltern der unbrauchbaren Daten\n",
    "\"\"\"\n",
    "\n",
    "def create_Geb_Reg(gdf): \n",
    "\n",
    "    # unary union fügt alle Polygone zu einem zusammen\n",
    "    merged_polygons = gdf.geometry.unary_union\n",
    "    geb_reg = gpd.GeoDataFrame(geometry=[merged_polygons], crs=\"EPSG:25832\")\n",
    "    \n",
    "    # explode wandelt alle mulitpart Polygone zu single part Polygonen um\n",
    "    geb_reg = geb_reg.explode(index_parts=True)\n",
    "\n",
    "    #Berechne die Größe der Gebäuderegion und hänge diese als \"SUM_Area\" an\n",
    "    geb_reg[\"SUM_AREA\"] = geb_reg.area\n",
    "    geb_reg[\"Reg_rich_comp\"] = (2 * sqrt(geb_reg.area * pi))/geb_reg.length\n",
    "    \n",
    "    #Filtern der nutzbaren Spalten, da sonst alle IP-Polygon Spalten übernommen werden\n",
    "    # welche nur noch unbrauchbare Daten beinhalten\n",
    "    geb_reg_filt = geb_reg[[\"geometry\", \"SUM_AREA\", \"Reg_rich_comp\"]]\n",
    "    \n",
    "    return geb_reg_filt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5da4108-c0e1-44be-89d0-b69bdeda1039",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funktion um die Statistischen Merkmale der bewohnten Häuser innerhalb der Gebäuderegion zu berechnen\n",
    "\n",
    "Merkmale: w_MIN_AREA, w_MAX_AREA\n",
    "Aufbau:\n",
    "1. Index resetten und festhalten für Rückgabe (\"merge\") der Werte auf ursprünglichen Datensatz\n",
    "2. Filtern der Nebengebäude\n",
    "3. Häuser auf ihre zugehörige Gebäuderegion joinen\n",
    "4. Gruppieren des Datensatzes pro Gebäuderegion\n",
    "5. kleinste und größte Fläche pro Gebäuderegion berechnen\n",
    "6. zurück-\"mergen\" auf ungefilterten Datensatz, festhalten der zugehörigen Gebäuderegion ID\n",
    "\"\"\"\n",
    "\n",
    "def geb_reg_stat(gdf, geb_reg):\n",
    "    \n",
    "    # Index wird resettet für spätere Merges, neue Variable muss festgelegt werden sonst entstehen Errors\n",
    "    gdf_res = gdf.reset_index(drop=False).rename(columns={'index': 'temp_idx'})\n",
    "    \n",
    "    #filtern der Nebengebäude für die w_ Statistiken\n",
    "    gdf_filt = gdf_res[gdf_res[\"HK_ANZ\"] >= 1]\n",
    "\n",
    "    # Sjoin der Gebäuderegionen auf die Gebäude um Id der Regionen zu erhalten für Groupen\n",
    "    # Gruppieren der Häuser nach ihrer jeweiligen Region\n",
    "    gdf_sjoin = gpd.sjoin(gdf_filt, geb_reg, how=\"left\", predicate=\"intersects\")\n",
    "    \n",
    "    #Gruppieren des Datensatzes nach ID der Gebäuderegion, welche automatisch als Feld \"index_right1\" angelegt wird\n",
    "    gdf_group = gdf_sjoin.groupby(\"index_right1\")\n",
    "    \n",
    "    \n",
    "    # Ermitteln der größten und kleinsten Gebäudegrößen der jeweiligen Gebäuderegion pro \"index_right1\"\n",
    "    # Ergenbiss sind PD Series, welche die min und max Werte pro Gebäuderegion enthalten\n",
    "    max_area = gdf_group[\"area\"].max()\n",
    "    min_area = gdf_group[\"area\"].min()\n",
    "    \n",
    "    # Umbenennen der Series zu den Spaltennamen für Ergebnis\n",
    "    MAX = max_area.rename(\"w_MAX_AREA\")\n",
    "    MIN = min_area.rename(\"w_MIN_AREA\")\n",
    "    \n",
    "    \n",
    "    #Zurück mergen der ermittelten Ergebnisse auf den Sjoin per \"index_right1\" und des jew. index\n",
    "    gdf_sjoin = gdf_sjoin.merge(MAX, left_on=\"index_right1\", right_index=True)\n",
    "    gdf_sjoin = gdf_sjoin.merge(MIN, left_on=\"index_right1\", right_index=True)\n",
    "\n",
    "\n",
    "    #Zurückmergen der Ergebnisse auf den ursprünglichen Datensatz mittels oben erstellten orig_idx Spalte, inplace sorgt dafür dass Datensatz nicht kopiert wird, sondern der ursprüngliche Verändert wird\n",
    "    #Id der Gebäuderregion wird festgehalten als Spalte für weitere Operationen\n",
    "    gdf_res = gdf_res.merge(gdf_sjoin[[\"temp_idx\",\"index_right1\", \"w_MAX_AREA\", \"w_MIN_AREA\", \"SUM_AREA\", \"Reg_rich_comp\"]], on=\"temp_idx\", how=\"left\")\n",
    "    gdf_res.drop(columns=[\"temp_idx\"], inplace=True)\n",
    "    gdf_res.rename(columns={\"index_right1\" : \"Reg_ID\"}, inplace=True)\n",
    "\n",
    "    return gdf_res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f449b35-b388-44ff-9471-44cabf31f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funktion zum Reklassifiezieren der ALKIS NRW \"Gebäudefunktionswerte\" in 3 Nutzungsklassen \n",
    "1=Wohnfunktion, 2=Wirtschafts und Industriegebäude, 3 = öffentliches Gebäude\n",
    "alle Gebäude mit partieller Wohnnutzung werden als Wohngebäude betrachtet\n",
    "\n",
    "Merkmal: Recl_Nutz\n",
    "\n",
    "Aufbau:\n",
    "1. Anlegen der neuen Spalte,\n",
    "2. .loc Funktion festlegen, welche die Werte entsprechend des Felds \"gebaeudefu\" reklassifiziert\n",
    "3. check für nicht reklassifizierte Werte\n",
    "\"\"\"\n",
    "\n",
    "def Reclass_GebFunkt(gdf):\n",
    "\n",
    "    # Anlegen der Reklassifizierten Nutzungsspalte\n",
    "    gdf[\"Recl_Nutz\"] = 0\n",
    "\n",
    "    # .loc Funktion greift auf Werte auf Basis der spezifizierten konditionen zu\n",
    "    # und weist die entsprechenden neuen Werte zu\n",
    "\n",
    "    gdf.loc[(gdf[\"gebaeudefu\"] <= 1312) | (gdf[\"gebaeudefu\"] == 2310) | (gdf[\"gebaeudefu\"] == 2320), \"Recl_Nutz\"] = 1\n",
    "    \n",
    "    gdf.loc[(gdf[\"gebaeudefu\"] >= 2000) & ((gdf[\"gebaeudefu\"] <= 2220) | ((gdf[\"gebaeudefu\"] >= 2400) & (gdf[\"gebaeudefu\"] <= 2740)) & (gdf[\"gebaeudefu\"] != 2463)), \"Recl_Nutz\"] = 2\n",
    "  \n",
    "    gdf.loc[(gdf[\"gebaeudefu\"] >= 3000) & (gdf[\"gebaeudefu\"] <= 3290), \"Recl_Nutz\"] = 3\n",
    "\n",
    "    #Neue Spalte zählen, um unklassifizierte Werte festzustellen\n",
    "    print(gdf[\"Recl_Nutz\"].value_counts())\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e62d8c7-df45-4f63-a183-c61e2d5e2ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funktion ermittelt die Anzahl von Punkten innerhalb eines Polygons\n",
    "\n",
    "Merkmal: HK_Anz\n",
    "\n",
    "Aufbau:\n",
    "1. Adressen auf zugehörige Häuser joinen\n",
    "2. GDF pro Gebäude gruppieren\n",
    "3. Gruppengröße zählen\n",
    "4. Rückgabe als Feld \"HK_ANZ\"\n",
    "5. n.a Werte (weil keine Adresse vorhanden ist) mit 0 ersetzen\n",
    "\"\"\"\n",
    "\n",
    "def count_points_in_polygons(points, gdf, polygon_id=\"gml_id\"):\n",
    "\n",
    "    # Zählen der Punkte in einem Polygon mittels räumlicher Verknüpfung, \n",
    "    # \"inner\" sorgt dafür, dass nur gezählt wird, falls Hausnummer vorliegt, \n",
    "    # \"groupby\" gibt Attribut vor, für welches gezählt wird, hier: Häuseridentifier\n",
    "    # \"size\" gibt die Anzahl der Punkte pro gml_id an, neue Spalte wird umbenannt\n",
    "    HK_pro_Haus = (gdf.sjoin(points, how=\"inner\").groupby(polygon_id).size().rename(\"HK_ANZ\"))\n",
    "\n",
    "    # Join der Hausnummern Anzahl auf die Häuser, für deren identifier \"gml_id\"\n",
    "    # \"left\" lässt die Geometrien der Punkte zurück, NoData/NAN werden mit 0 gefüllt\n",
    "    result_gdf = (gdf.join(HK_pro_Haus,on=polygon_id, how=\"left\").fillna({\"HK_ANZ\": 0}))\n",
    "\n",
    "    return result_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1c192b1-22d7-4f1a-b2aa-d46fdf3495aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "die Funktion berechnet die Standartabweichungen der Gebäudegrößen pro Gebäudeblock (target_pol) ohne Nebengebäude\n",
    "\n",
    "Merkmal: w_Std_Area\n",
    "\n",
    "Aufbau:\n",
    "1. Filtern der Nebengebäude\n",
    "2. Häuser pro Gebäudeblock gruppieren\n",
    "3. Standartabweichung der Gebäudegrößen berechnen\n",
    "4. Daten zurückgeben an IP GDF\n",
    "\"\"\"\n",
    "\n",
    "def w_STD_AREA(ip_gdf): #falls nötig (ip_gdf, target_pol):\n",
    "\n",
    "    #ip_gdf muss auf Nebengebäude gefiltert werden, da w_merkmale ohne Nebengebäude sind\n",
    "    ip_gdf_filter = ip_gdf[ip_gdf[\"HK_ANZ\"] >= 1]\n",
    "    \n",
    "    # area Spalte des IP schon vrohanden, deshalb gruppieren pro Block und die Std der Größe ermitteln,\n",
    "    # um Ergebnisse nicht zu verfälschen durch verschnittene Häuser wird die ganze Größe eines Hauses in einem Block betrachtet\n",
    "    grouped = ip_gdf_filter.groupby(\"block_id\")\n",
    "    std_areas = grouped[\"area\"].std()\n",
    "    \n",
    "    # Dataframe wird mit ursprünglichem IP gemergt um Werte zurück auf Input zu spiegeln\n",
    "    # gemergt wird per ermittelter block_id, auf den Index des Dataframes\n",
    "    ip_gdf = ip_gdf.merge(std_areas.rename(\"w_STD_AREA\"), left_on=\"block_id\", right_on=\"block_id\", how=\"left\")\n",
    "\n",
    "    return ip_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "404aada6-40fa-4593-ba6f-b5c5524f2b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funktion zur Berechnung des Flächenanteils aller Polygone (Häuser) in einem größeren Polygon (Baublock)\n",
    "Flächenanteil wird auf alle Häuser bezogen zurückgegeben und nicht einzeln\n",
    "\n",
    "Merkmal: BLABERAT\n",
    "\n",
    "Aufbau:\n",
    "1. Größe des Gebäudeblocks berechnen\n",
    "2. Häuser auf zugehörige Blocks joinen\n",
    "3. Gruppieren pro Block Index\n",
    "4. Fläche der Häuser in Gebäudeblock berechnen\n",
    "5. durch die Fläche des Blocks teilen\n",
    "6. Anteil zurückgeben an Häuserdatensatz\n",
    "\"\"\"\n",
    "\n",
    "def summarize_polygon_area_within_polygons(polygons, target_polygons):\n",
    "\n",
    "    # Calculate the area of each polygon in the target_polygons GeoDataFrame\n",
    "    target_polygons[\"area_block\"] = target_polygons.area\n",
    "\n",
    "    #Spatial join, der die Häuser zu den Blocks matched. um alle Häuser zu erhalten wird left festgelegt. \n",
    "    # Aufgrund von Häusern die in mehreren blocks liegen muss \"intersect\" gewählt werden, sodass die Polygone in beide gematcht werden\n",
    "    overlapping_polygons = gpd.sjoin(polygons, target_polygons, how=\"left\", predicate=\"intersects\")\n",
    "\n",
    "    # Polygone des Spatial joins werden nach Block gruppiert\n",
    "    grouped_polygons = overlapping_polygons.groupby(\"index_right\")\n",
    "\n",
    "    # Aufgrund der Polygon, die in mehreren Blocks vorliegen muss die überschneidende Area neuberechnet werden, \n",
    "    # um die korrekten Anteile der überlappenden Parts festzustellen, Sum Areas ethält die intersecting Area der Polygone pro Baublock ID\n",
    "    sum_areas = grouped_polygons[\"geometry\"].apply(lambda x: x.unary_union.area)\n",
    "\n",
    "    # um bei der Proportionen berechnung die Indices eins zu eins zu matchen werden die Größen der Blocks gruppiert nach ihrem Index \n",
    "    area_blocks = target_polygons.groupby(target_polygons.index)[\"area_block\"]\n",
    "    \n",
    "    # Berechnung der Proportionen der Intersection Fläche an der Blockfläche\n",
    "    proportions = sum_areas / grouped_polygons[\"area_block\"].first()\n",
    "\n",
    "    # Die Proportionen Series wird in einen Dataframe umgewandelt um sie später zruückzu mergen\n",
    "    proportions_df = pd.DataFrame(proportions, columns=[\"BLABERAT\"])\n",
    "\n",
    "    # Reseten des Index um dieses als Spalte zu bekommen, Inplace erlaubt, dass dies im gleichen Datensatz vorgeht\n",
    "    proportions_df.reset_index(inplace=True)\n",
    "\n",
    "    # Proportionen werden auf die Sjoin Polygone zurückgespiegelt, da dort der Index vorhanden ist\n",
    "    overlapping_polygons = overlapping_polygons.merge(proportions_df, on=\"index_right\", how=\"left\")\n",
    "\n",
    "    return overlapping_polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "98c35c8b-9eeb-4284-b726-c390f73a3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Kalkuliert die Richardson Compactness (Rundheitsmaß) für den Input\n",
    "Rich_comp = 2 * sqrt(pi * area) / perimeter\n",
    "\"\"\"\n",
    "\n",
    "def richardson_compactness(gdf):\n",
    "\n",
    "    gdf[\"rich_comp\"] = (2 * sqrt(gdf[\"area\"] * pi))/gdf[\"length\"]\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed55193d-bbdc-49b0-bc92-8f2050e7dcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Funktion kalkuliert das Volumen und Höche eines Polygons, \n",
    "indem es die Rasterwerte eines normalisierten DEMs in den Polygongrenzen aufsummiert\n",
    "\n",
    "Merkmal: vol, height\n",
    "\n",
    "Aufbau:\n",
    "1. Anlegen von Listen, welche die Werte speichern\n",
    "2. NDOM importieren\n",
    "3. Raster auf Polygone zuschneiden\n",
    "4. Höhe und Voloumen des Gebäude ermitteln, Werte in Listen speichern\n",
    "5. Listen als Spalten des Datensatzes speichern\n",
    "\"\"\"\n",
    "\n",
    "def calc_vol(gdf, ndem): \n",
    "\n",
    "    #Erstellen einer leeren Liste, welche die Volumen speichert und einer für die Höhe\n",
    "    volumes = []\n",
    "    heights = []\n",
    "    \n",
    "    #import NDOM\n",
    "    with rasterio.open(ndem) as ip_raster:\n",
    "\n",
    "        #Loop über alle Polygone\n",
    "        for index, row in gdf.iterrows():\n",
    "\n",
    "            # extrahieren der geometrien für Maske\n",
    "            geom = row[\"geometry\"]\n",
    "            \n",
    "            # Raster wird per Maske auf geom geclipt (crop=True), \n",
    "            # additional OP der funktion wird nicht benötigt deswegen _\n",
    "            clip_raster, _ = mask(ip_raster, [geom], crop=True)\n",
    "\n",
    "            # Die geklippten Rasterwerte werden aufsummiert, was dem Volumen des Gebäude entspricht\n",
    "            # Der durchschnittliche Rasterwert entspricht der Gebäudehöhe\n",
    "            vol = clip_raster.sum()\n",
    "            height_val = float(clip_raster.mean())\n",
    "\n",
    "            #Die Volumen und Höhen werden der Liste angefügt\n",
    "            volumes.append(vol)\n",
    "            heights.append(height_val)\n",
    "            \n",
    "    \n",
    "    #Höhen und Volumen werden zurückgegeben an Häuserdatensatz\n",
    "    gdf.loc[:, \"vol\"] = volumes\n",
    "    gdf.loc[:, \"height\"] = heights\n",
    "    \n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7b86641-5499-4027-b2b9-90f864bc4d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nils\\anaconda3\\envs\\geopandas\\Lib\\site-packages\\geopandas\\array.py:1459: UserWarning: CRS not set for some of the concatenation inputs. Setting output's CRS as ETRS89 / UTM zone 32N (the single non-null crs provided).\n",
      "  return GeometryArray(data, crs=_get_common_crs(to_concat))\n"
     ]
    }
   ],
   "source": [
    "#Import der Basisdaten, Filtern der Kreise auf UG per Name\n",
    "\n",
    "ip_kreise = gpd.read_file(r\"F:\\Basisdaten\\dvg1krs_nw.shp\", crs=\"EPSG:25832\")\n",
    "UG_kreise = ip_kreise[ip_kreise['GN'].isin([ \"Ennepe-Ruhr-Kreis\",  \"Oberbergischer Kreis\", \"Wuppertal\", \"Remscheid\" ])]\n",
    "adressen = gpd.read_file(r\"F:\\Basisdaten\\Geb_ref_UG.shp\")\n",
    "ip_Baublock = gpd.read_file(r\"F:\\Basisdaten\\Baublock_UG.shp\", crs=\"EPSG:25832\")\n",
    "\n",
    "#import der ALKIS Daten des UG\n",
    "alk_wup = gpd.read_file(r\"F:\\Basisdaten\\ALKIS\\AX_gebauede.shp\", crs=\"EPSG:25832\")\n",
    "alk_rem = gpd.read_file(r\"F:\\Basisdaten\\ALKIS\\ERK_alk.shp\", crs=\"EPSG:25832\")\n",
    "alk_obk = gpd.read_file(r\"F:\\Basisdaten\\ALKIS\\OBK_alkis.shp\", crs=\"EPSG:25832\")\n",
    "alk_erk = gpd.read_file(r\"F:\\Basisdaten\\ALKIS\\Rem_alkis.shp\", crs=\"EPSG:25832\")\n",
    "\n",
    "#Merge der ALKIS Datasets zu einem einzigen\n",
    "ALKIS_UG = gpd.GeoDataFrame(pd.concat([alk_wup, alk_rem, alk_obk, alk_erk]), crs=\"EPSG:25832\")\n",
    "\n",
    "#Reset des indexes um Duplikate zu vermeiden, drop=True alter index wird gelöscht, inplace=True Datensatz wird für default=false kopiert\n",
    "ALKIS_UG.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13ad2210-0e49-409a-998f-42c520f8ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Droppen aller überflüssigen Daten, die im Datensatz vorhanden sind\n",
    "important_col = [\"gml_id\", \"gebaeudefu\"]\n",
    "ALKIS_UG_cl = gpd.GeoDataFrame(ALKIS_UG[important_col], geometry=ALKIS_UG.geometry, crs=ALKIS_UG.crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c5bcecd3-fd04-42f2-a822-0ac715f58a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anwendung aller definierten Funktionen um die Merkmale für die Klassifikation zu generieren\n",
    "# Zuerst alle Merkmale, die die Nebengebäude miteinbeziehen\n",
    "# Schritt 1: kalkulieren von Größe und Umfang\n",
    "alk_1 = calc_area_length(ALKIS_UG_cl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6579986-f1d2-40ad-8ca8-8056b92f4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 2: Ermittlung der Anzahl der Hausadressen pro Gebäude, \n",
    "# erlaubt später Filtern der Nebengebäude durch Auswahl aller Gebäude mit mindestens einer Adresse\n",
    "alk_2 = count_points_in_polygons(adressen, alk_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30600fb8-06a4-4367-a371-e3304bd16f6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Schritt 3: Ermittlung der Anzahl der Gebäude mit gemeinsamer Kante\n",
    "alk_3 = alt_Sum_AnzPo(alk_2, \"SUM_AnzPO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "810fcaa4-c99a-4407-84f4-c69b6fc08e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 4: Berechnung des Blockbezogenen Gebäudeanteils, \n",
    "# 7 NAN Werte ,da sie ausserhakb der Baublöcke liegen und keine Intersection aufweisen \n",
    "alk_4 = summarize_polygon_area_within_polygons(alk_3, ip_Baublock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c9d9a79-b962-495f-9ce7-ac6edf8e9ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming von \"index_right\" zu Block_id für Klarheit, im Datensatz\n",
    "alk_4.rename(columns={'index_right': 'block_id'}, inplace=True)\n",
    "# Removen der 7 Gebäude ausserhald von Blockgrenzen\n",
    "alk_4.dropna(subset=[\"block_id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63abd864-2be6-4008-92fc-66a3ec2c79f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 5: Errechnen der Richardson Compactness\n",
    "alk_5 = richardson_compactness(alk_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c1b9aed-fbda-42b5-a601-15412647859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Berechnung aller Merkmale die auf Nebengebäude verzichten\n",
    "\n",
    "# Schritt 6: Berechnung der Std der Häusergrößen ohne Nebengebäude\n",
    "\n",
    "alk_6 = w_STD_AREA(alk_5)\n",
    "\n",
    "# Datensatz enthält NaN Werte für alle Baublöcke welche nur ein Polygon/Haus enthalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d069a29-cc9b-4d80-93ca-85ea27ecc733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 7: Berechnung Anzahl der benachbarten Gebäude mit gemeinsamer Kante ohne Nebengebäude\n",
    "alk_7 = w_Sum_AnzPo(alk_6, \"w_SUM_AnzPo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6fdc4a38-0e95-4a85-8bcb-467412e64310",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Reset des Index, da dieser durch \"w_Sum_AnzPo\" NAN und 0 Werte aufweist\n",
    "alk_7.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3ab6022-6f03-46e6-8ff1-4ed165715dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstellen der Gebäuderegionen\n",
    "geb_reg = create_Geb_Reg(alk_7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0dfeb4e-46b9-41ed-98a9-f2fb18e2b778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schritt 8: Erstellen der Gebäuderegion spezifischen Statistiken auf Basis der Häuserdaten\n",
    "alk_8 = geb_reg_stat(alk_7, geb_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f58f1175-a275-47d4-8301-940f6417d0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recl_Nutz\n",
      "1    270233\n",
      "2    145335\n",
      "0     97775\n",
      "3      6949\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Schritt 9: Reklassifizieren der Gebäudefunktion, filtern von ungültigen Werten\n",
    "alk_9 = Reclass_GebFunkt(alk_8)\n",
    "alk_10 = alk_9[alk_9[\"Recl_Nutz\"] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e2f4e40-b326-40cf-86b2-2868c7970e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nils\\anaconda3\\envs\\geopandas\\Lib\\site-packages\\geopandas\\geodataframe.py:1543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n",
      "C:\\Users\\Nils\\anaconda3\\envs\\geopandas\\Lib\\site-packages\\geopandas\\geodataframe.py:1543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  super().__setitem__(key, value)\n"
     ]
    }
   ],
   "source": [
    "# Schritt 10: Errechnen des Volumen und der Höhe für jedes Gebäude,\n",
    "ndom_ug = r\"F:\\Basisdaten\\ndom_merge.tif\"\n",
    "alk_11 = calc_vol(alk_10, ndom_ug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0bb9df49-473b-422e-b68f-03b4f2d87a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Schritt 11: Fallen lassen aller Zeilen, welche unvollständige Daten aufweisen\n",
    "\n",
    "# Liste festlegen mit collumns die keine NaN Werte haben dürfen für Klassifikation, \n",
    "clean_col = [\"w_STD_AREA\", \"BLABERAT\", \"rich_comp\"]\n",
    "# Fallen lassen aller NaN Werte, da Random Forest nicht mit NoData umgehen kann\n",
    "Merkmale_clean = alk_11.dropna(subset=clean_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d53f8efa-5aa2-4f6b-a5af-eb896f79faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Festlegen der Columns, welche unverwendbare Daten enthält\n",
    "columns_drop = [\"ART\", \"GN\", \"KN\", \"STAND\", \"FID_gem_UG\", \"oid_1\", \"aktualit\", \"art_1\", \"name\", \"schluessel\", \"gemeinde\", \"gmdschl\", \"stadtteil\", \"block\", \"gid\", \"objectid_1\", \"refname\", \"FID_line_t\", \"FID_OBK_ER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e8e44a0-3d4d-4586-b4a3-414fb5ece811",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nils\\AppData\\Local\\Temp\\ipykernel_2940\\1716140455.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Merkmale_clean.drop(columns=columns_drop, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Aufräumen des Datensatzes, Spalten aus festgelegter Liste droppen\n",
    "Merkmale_clean.drop(columns=columns_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3f280660-1403-4267-9b5d-0a3005794d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nils\\AppData\\Local\\Temp\\ipykernel_2940\\979086632.py:2: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  Merkmale_clean.to_file(r\"F:\\Zwischendaten\\Data_Hausmerkmale.shp\")\n"
     ]
    }
   ],
   "source": [
    "# Export des Datensatzes für Weiterverwendung in ARCGIS\n",
    "Merkmale_clean.to_file(r\"F:\\Zwischendaten\\Data_Hausmerkmale.shp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92322ec9-08b1-4ebb-89bc-34bb47950a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc439a2d-3fbd-49f7-80bf-ba89651be421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4134c0d6-9b92-4175-a728-9d0f66d4642d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
